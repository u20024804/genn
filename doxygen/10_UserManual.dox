/*--------------------------------------------------------------------------
   Author: Thomas Nowotny
  
   Institute: Center for Computational Neuroscience and Robotics
              University of Sussex
	      Falmer, Brighton BN1 9QJ, UK 
  
   email to:  T.Nowotny@sussex.ac.uk
  
   initial version: 2010-02-07
  
--------------------------------------------------------------------------*/


//----------------------------------------------------------------------------
/*!  \page UserManual User Manual

\section Contents

\ref sIntro <br />
\subpage sect1 <br />
\subpage sect2 <br />
\subpage sect3 <br />
\subpage sect_postsyn <br />
\subpage ListOfVariables <br />
\subpage Credits <br />

\section sIntro Introduction
GeNN is a software library for facilitating the simulation of neuronal
network models on NVIDIA CUDA enabled GPU hardware. It was designed
with computational neuroscience models in mind rather than artificial
neural networks. The main philosophy of GeNN is two-fold:
1. GeNN relies heavily on code generation to make it very flexible and
to allow adjusting simulation code to the model of interest and the GPU
hardware that is detected at compile time.
2. GeNN is lightweight in that it provides code for running models of
neuronal networks on GPU hardware but it leaves it to the user to
write a final simulation engine. It so allows maximal flexibility to
the user who can use any of the provided code but can fully choose,
inspect, extend or otherwise modify the generated code. She can also
introduce her own optimisations and in particular control the data
flow from and to the GPU in any desired granularity.

This manual gives an overview of how to use GeNN for a novice user and
tries to lead the user to more expert use later on. With this we jump
right in.

<br />

-----
\link ReleaseNotes Previous\endlink | \link UserManual Top\endlink | \link sect1 Next\endlink
*/
*/

//----------------------------------------------------------------------------
/*!  
\page sect1 Defining a network model
A network model is defined by the user by providing the function 
\code{.cc}
void modelDefinition(NNmodel &model) 
\endcode
in a separate file with name `name.cc`, where `name` is the name of the model network under consideration. In this function, the following tasks must be completed:
1. The name of the model must be defined:
\code{.cc}
model.setName("MyModel");
\endcode
2. Neuron populations (at least one) must be added (see \ref subsect11). The
  user may add as many neuron populations as she wishes. If resources
  run out, there will not be a warning but GeNN will fail. However, before this breaking point is reached, GeNN will make all necessary efforts in terms of block size optimisation to accommodate the defined models. All
  populations should have a unique name.

3. Synapse populations (zero or more) can be added (see \ref subsect12). Again,
  the number of synaptic connection populations is unlimited other than
  by resources.

\note
GeNN uses the convention where C/C++ files end in `.cc`. If this is not adhered to the build script `buildmodel.sh` will not recognise the model definition file.

\section subsect11 Defining neuron populations

Neuron populations are added using the function
\code{.cc}
model.addNeuronPopulation(name, n, TYPE, para, ini);
\endcode
where the arguments are:
\arg `const char* name`: Name of the neuron population
\arg `int n`: number of neurons in the population
\arg `int TYPE`: Type of the neurons, refers to either a standard type (see \ref sect2) or user-defined type
\arg `double *para`: Parameters of this neuron type
\arg `double *ini`: Initial values for variables of this neuron type 

The user may add as many neuron populations as the model necessitates. They should all have unique names. The possible values for the arguments, predefined models and their parameters and initial values are detailed \ref sect2 below.

\section subsect12 Defining synapse populations

Synapse populations are added with the command
\code{.cc}
model.addSynapsePopulation("name", sType, sConn, gType, delay, postSyn, "preName", "postName", sIni, sParam, postSynIni, postSynParam);
\endcode
where the arguments are
\arg \c const \c char* name: The name of the synapse population
\arg \c int \c sType: The type of synapse to be added. See \ref subsect31 below for the available predefined synapse types.
\arg \c int \c sConn: The type of synaptic connectivity. the options currently are "ALLTOALL",
"DENSE", "SPARSE" (see \ref subsect32 )
\arg \c int \c gType: The way how the synaptic conductivity g will be
defined. Options are
"INDIVIDUALG", "GLOBALG", "INDIVIDUALID". For their meaning, see \ref
subsect33 below.
\arg \c int \c delay: Synaptic delay (in multiples of the simulation time step `DT`). 
\arg \c int \c postSyn: Postsynaptic integration method. See \ref sect_postsyn for predefined types.
\arg \c char* \c preName: Name of the (existing!) pre-synaptic neuron
population.
\arg \c char* \c postName: Name of the (existing!) post-synaptic neuron
population.
\arg `double *sIni`: A C-array of doubles containing initial values for the (pre-) synaptic variables. 
\arg `double *sParam`: A C-array of double precision that contains
parameter values (common to all synapses of the population) which will
be used for the defined synapses. The array must contain the right
number of parameters in the right order for the chosen synapse
type. If too few, segmentation faults will occur, if too many, excess
will be ignored. For pre-defined synapse types the required parameters and their meaning are listed in \ref sect31 below. 
\arg \c double * \c psIni: A C-array of double precision numbers containing initial values for the post-synaptic model variables
\arg \c double * \c psPara: A C-array of double precision numbers containing parameters fo the post-snaptic model.
\note
If the synapse conductance definition type is "GLOBALG" then the
global value of the synapse conductances is taken from the initial value provided in `sINI`. (The function setSynapseG() from earlier versions of GeNN has been deprecated).

Synaptic updates can occur per "true" spike (i.e at one point per spike, e.g. after a threshold was crossed) or for all "spike type events" (e.g. all points above a given threshold). This is defined within each given synapse type.

<br />

-----
\link UserManual Previous\endlink | \link sect1 Top\endlink | \link sect3 Next\endlink
*/

//----------------------------------------------------------------------------
/*! 
\page sect2 Neuron models
There is a number of predefined models which can be chosen in the \c
addNeuronGroup(...) function by their unique cardinal number, starting from
0. For convenience, C variables with readable names are predefined
- 0: \ref sect21 "MAPNEURON" 
- 1: \ref sect22 "POISSONNEURON"
- 2: \ref sect23 "TRAUBMILES"
- 3: \ref sect24 "IZHIKEVICH"
- 4: \ref sect25 "IZHIKEVICH_V"

\note
Ist is best practice to not depend on the unique cardinal numbers but use predefined names. While it is not intended that the numbers will change the unique names are guaranteed to work in all future versions of GeNN.

\section sect21 MAPNEURON (Map Neurons)
The MAPNEURON type is a map based neuron model based on \cite Rulkov2002 but in the 1-dimensional map form used in \cite nowotny2005self : 
\f{eqnarray*}{
V(t+\Delta t) &=& \left\{ \begin{array}{ll}
V_{\rm spike} \Big(\frac{\alpha V_{\rm spike}}{V_{\rm spike}-V(t) \beta I_{\rm syn}} + y \Big) & V(t) \leq 0 \\
V_{\rm spike} \big(\alpha+y\big) & V(t) \leq V_{\rm spike} \big(\alpha + y\big) \; \& \; V(t-\Delta t) \leq 0 \\
-V_{\rm spike} & {\rm otherwise}
\end{array}
\right.
\f}
\note
The `MAPNEURON` type only works as intended for the single time step size of `DT`= 0.5.

The `MAPNEURON` type has 2 variables:
- \c V - the membrane potential
- \c preV - the membrane potential at the previous time step

and it has 4 parameters:
- \c Vspike - determines the amplitude of spikes, typically -60mV
- \c alpha - determines the shape of the iteration function, typically \f$\alpha \f$= 3
- \c y - "shift / excitation" parameter, also determines the iteration function,originally, y= -2.468 
- \c beta - roughly speaking equivalent to the input resistance, i.e. it regulates the scale of the input into the neuron, typically \f$\beta\f$= 2.64 \f${\rm M}\Omega\f$.

\note
The initial values array for the `MAPNEURON` type needs two entries for `V` and `Vpre` and the parameter array needs four entries for `Vspike`, `alpha`, `y` and `beta`,  *in that order*.

\section sect22 POISSONNEURON (Poisson Neurons)

Poisson neurons have constant membrane potential (\c Vrest) unless they are 
activated randomly to the \c Vspike value if (t- \c SpikeTime ) > \c trefract.

It has 3 variables:

- \c V - Membrane potential
- \c Seed - Seed for random number generation
- \c SpikeTime - Time at which the neuron spiked for the last time

and 4 parameters:

- \c therate - Firing rate
- \c trefract - Refractory period
- \c Vspike - Membrane potential at spike (mV)
- \c Vrest - Membrane potential at rest (mV)

\note
The initial values array for the `POISSONNEURON` type needs three entries for `V`, `Seed` and `SpikeTime` and the parameter array needs four entries for `therate`, `trefract`, `Vspike` and `Vrest`,  *in that order*.

\note Internally, GeNN uses a linear approximation for the probability
of firing a spike in a given time step of size `DT`, i.e. the
probability of firing is `therate` times `DT`: \f$ p = \lambda \Delta t
\f$. This approximation is usually very good, especially for typical,
quite small time steps and moderate firing rates. However, it is worth
noting that the approximation becomes poor for very high firing rates
and large time steps. An unrelated problem may occur with very low
firing rates and small time steps. In that case it can occur that the
firing probability is so small that the granularity of the 64 bit
integer based random number generator begins to show. The effect
manifests itself in that small changes in the firing rate do not seem
to have an effect on the behaviour of the Poisson neurons because the
numbers are so small that only if the random number is identical 0 a
spike will be triggered.
 
\section sect23 TRAUBMILES (Hodgkin-Huxley neurons with Traub & Miles algorithm)

This conductance based model has been taken from \ref Traub1991 and can be described by the equations:
\f{eqnarray*}{
C \frac{d V}{dt}  &=& -I_{{\rm Na}} -I_K-I_{{\rm leak}}-I_M-I_{i,DC}-I_{i,{\rm syn}}-I_i, \\
I_{{\rm Na}}(t) &=& g_{{\rm Na}} m_i(t)^3 h_i(t)(V_i(t)-E_{{\rm Na}}) \\
I_{{\rm K}}(t) &=& g_{{\rm K}} n_i(t)^4(V_i(t)-E_{{\rm K}})  \\
\frac{dy(t)}{dt} &=& \alpha_y (V(t))(1-y(t))-\beta_y(V(t)) y(t), \f}
where \f$y_i= m, h, n\f$, and
\f{eqnarray*}{
\alpha_n&=& 0.032(-50-V)/\big(\exp((-50-V)/5)-1\big)  \\
  \beta_n &=& 0.5\exp((-55-V)/40)  \\
  \alpha_m &=& 0.32(-52-V)/\big(\exp((-52-V)/4)-1\big)  \\
   \beta_m &=& 0.28(25+V)/\big(\exp((25+V)/5)-1\big)  \\
   \alpha_h &=& 0.128\exp((-48-V)/18)  \\
   \beta_h &=& 4/\big(\exp((-25-V)/5)+1\big). 
\f}
and typical parameters are \f$C=0.143\f$ nF, \f$g_{{\rm leak}}= 0.02672\f$
\f$\mu\f$S, \f$E_{{\rm leak}}= -63.563\f$ mV, \f$g_{{\rm Na}}=7.15\f$ \f$\mu\f$S,
\f$E_{{\rm Na}}= 50\f$ mV, \f$g_{{\rm {\rm K}}}=1.43\f$ \f$\mu\f$S,
\f$E_{{\rm K}}= -95\f$ mV. 

It has 4 variables:

- \c V - membrane potential E
- \c m - probability for Na channel activation m
- \c h - probability for not Na channel blocking h
- \c n - probability for K channel activation n

and 7 parameters:

- \c gNa - Na conductance in 1/(mOhms * cm^2)
- \c ENa - Na equi potential in mV
- \c gK - K conductance in 1/(mOhms * cm^2)
- \c EK - K equi potential in mV
- \c gl - Leak conductance in 1/(mOhms * cm^2)
- \c El - Leak equi potential in mV
- \c Cmem - Membrane capacity density in muF/cm^2

\note
Internally, the ordinary differential equations defining the model are integrated with a linear Euler algorithm and GeNN integrates 25 internal time steps for each neuron for each network time step. I.e., if the network is simulated at `DT= 0.1` ms, then the neurons are integrated with a linear Euler algorithm with `lDT= 0.004` ms.

\section sect24 IZHIKEVICH (Izhikevich neurons with fixed parameters)
This is the Izhikevich model with fixed parameters \cite izhikevich2003simple.
It is usually described as 
\f{eqnarray*}
\frac{dV}{dt} &=& 0.04 V^2 + 5 V + 140 - U + I, \\
\frac{dU}{dt} &=& a (bV-U),
\f}
I is an external input current and the voltage V is reset to parameter c and U incremented by parameter d, whenever V >= 30 mV. This is paired with a particular integration procedure of two 0.5 ms Euler time steps for the V equation followed by one 1 ms time step of the U equation. Because of its popularity we provide this model in this form here event though due to the details of the usual implementation it is strictly speaking inconsistent with the displayed equations.

Variables are:

- \c V - Membrane potential
- \c U - Membrane recovery variable

Parameters are:
- \c a - time scale of U
- \c b - sensitivity of U
- \c c - after-spike reset value of V
- \c d - after-spike reset value of U

\section sect25 IZHIKEVICH_V (Izhikevich neurons with variable parameters) 
This is the same model as \ref sect24 IZHIKEVICH but
parameters are defined as "variables" in order to allow users to provide individual values for each individual neuron instead of fixed values for all neurons across the population.

Accordingly, the model has the Variables:
- \c V - Membrane potential
- \c U - Membrane recovery variable
- \c a - time scale of U
- \c b - sensitivity of U
- \c c - after-spike reset value of V
- \c d - after-spike reset value of U

and no parameters.


\section sect_own Defining your own neuron type 

In order to define a new neuron type for use in a GeNN application,
it is necessary to populate an object of class \c neuronModel and
append it to the global vector \c nModels. The \c neuronModel class
has several data members that make up the full description of the
neuron model:

- \c simCode of type \c string: This needs to be assigned a C++ string
  that contains the code for executing the integration of the model
  for one time step. Within this code string, variables need to be
  referred to by $(NAME), where NAME is the name of the variable as
  defined in the vector varNames. The code may refer to the predefined
  primitives `DT` for the
  time step size and `Isyn` for the total incoming synaptic current. <br />
  Example:
\code
neuronModel model;
model.simCode=String("$(V)+= (-$(a)$(V)+$(Isyn))*DT;");
\endcode
would implement a leaky itegrator \f$\frac{dV}{dt}= -a V + I_{{\rm syn}}\f$.

- \c varNames of type \c vector<string>: This vector needs to be
  filled with the names of variables that make up the neuron
  state. The variables defined here as `NAME` can then be used in the
  syntax `$(NAME)` in the code string. <br />
  Example:
\code
model.varNames.push_back(String("V"));
\endcode
would add the variable V as needed by the code string in the example above.

- \c varTypes of type \c vector<string>: This vector needs to be
  filled with the variable type (e.g. "float", "double", etc) for the
  variables defined in \c varNames. Types and variables are matched to
  each other by position in the respective vectors. <br />
Example:
\code
model.varTypes.push_back(String("float"));
\endcode
would designate the variable V to be of type float. 
\note
Variable names and variable types are matched by their position in the corresponding arrays `varNames` and `varTypes`, i.e. the 0th entry of `varNames` will have the type stored in the 0th entry of `varTypes` and so on.

- \c pNames of type \c vector<string>: This vector will contain the names
 of parameters relevant to the model. If defined as `NAME` here, they
 can then be referenced as `$(NAME)` in the code string. The length of
 this vector determines the expected number of parameters in the
 initialisation of the neuron model. Parameters are currently assumed
 to be always of type double. <br />
 \code
model.pNames.push_back(String("a"));
\endcode
stores the parameter `a` needed in the code example above.

- \c dpNames of type \c vector<string>: Names of "dependent
  parameters". Dependent parameters are a mechanism for enhanced efficiency when running
  neuron models. If parameters with model-side meaning, such as time
  constants or conductances always appear in a certain combination in
  the model, then it is more efficient to pre-compute this combination
  and define it as a dependent parameter. This vector contains the
  names of such dependent parameters. <br />
For example, if in the example above the original model had been \f$\frac{dV}{dt} = -g/C \, V +I_{\rm syn}\f$. Then one could define the code string and parameters as
\code
model.simCode=String("$(V)+= (-$(a)$(V)+$(Isyn))*DT;");
model.varNames.push_back(String("V"));
model.varTypes.push_back(String("float"));
model.pNames.push_back(String("g"));
model.pNames.push_back(String("C"));
model.dpNames.push_back(String("a"));
\endcode

- `dps` of type \ref `dpclass`*: The dependent parameter class, i.e. an implementation of \ref `dpclass` which would return the value for dependent parameters when queried for them. E.g. in the example above it would return `a` when queried for dependent parameter 0 through `dpclass::calculateDerivedParameter()`. Examples how this is done can be found in the pre-defined classes, e.g. expDecayDp, pwSTDP, \ref rulkovdp etc. 

- \c tmpVarNames of type \c vector<string>: This vector can be used to
  request additional variables that are not part of the state of the
  neuron but used only as temporary storage during evaluation of the
  integration time step. <br />
For example, one could define
\code
model.tmpVarNames.push_back(String("a"));
model.tmpVarNames.push_back(String("float"));
model.simCode= String("$(a)= $(g)/$(C); \n\
$(V)+= -$(a)*$(V);\n\");
\endcode
which would be equivalent to
\code
model.simCode= String("float $(a)= $(g)/$(C); \n\
$(V)+= -$(a)*$(V);\n\");
\endcode

- \c tmpVarTypes of type \c vector<string>: This vector will contain
  the variable types of the temporary variables, matched up by position as usual.

Once the completed \c neuronModel object is appended to the \c nModels
vector,
\code
nModels.push_back(model);
\endcode
 it can be used in network descriptions by referring to its
cardinal number in the nModels vector. I.e., if the model is added as
the 4th entry, it would be model "3" (counting starts at 0 in usual C
convention). The information of the cardinal number of a new model can
be obtained by referring to \c nModels.size() right before appending
the new model, i.e. a typical use case would be.
\code
int myModel= nModels.size();
nModels.push_back(model);
\endcode.
Then one can use the model as
\code
networkModel.addNeuronPopulation(..., myModel, ...);
\endcode

\section sect_explinput Explicit current input to neurons 
The user can decide whether a neuron group receives external input current or not 
in addition to the synaptic input that it receives from the network.
External input to a neuron group is activated by calling 
\c activateDirectInput function. It receives two arguments: The first argument is 
the name of the neuron group to receive input current. The second parameter defines 
the type of input. Current options are:

- 0: NOINP: Neuron group receives no input. This is the value by default
 when the explicit input is not activated.

- 1: CONSTINP: All the neurons receive a constant input value. This value may also be defined as a variable, i.e. be time-dependent.

- 2: MATINP: The input is read from a file containing the input matrix.

- 3: INPRULE: The input is defined as a rule. It differs from CONSTINP
in the way that the input is injected: CONSTINP creates a value which
is modified in real time, hence complex instructions are
limited. INPRULE creates an input array which will be copied into the
device memory.
*/

//----------------------------------------------------------------------------
/*! 
\page sect3 Synapse models
\section subsect31 Models
Currently 3 predefined synapse models are available: 
- 0: \ref sect31 "NSYNAPSE" 
- 1: \ref sect32 "NGRADSYNAPSE"
- 2: \ref sect33 "LEARN1SYNAPSE"

\section sect31 NSYNAPSE (No Learning)
If this model is selected, no learning rule is applied to the synapse. 
The model has 3 parameters:
- \c tau_S* - decay time constant for S [ms] 
- \c Epre: Presynaptic threshold potential
- \c Erev* - Reversal potential

(* DEPRECATED: Decay time constant and reversal potential in synapse parameters are not used anymore and they will be removed in the next release. They should be defined in the postsynaptic mechanisms.)

\section sect32 NGRADSYNAPSE (Graded Synapse)
In a graded synapse, the conductance is updated gradually with the rule: 

\f[ gSyn= g * tanh((E - E_{pre}) / V_{slope} \f]

The parameters are:
- \c Erev*: Reversal potential
- \c Epre: Presynaptic threshold potential 
- \c tau_S*: Decay time constant for S [ms]
- \c Vslope: Activation slope of graded release 

(* DEPRECATED: Decay time constant and reversal potential in synapse parameters are not used anymore and they will be removed in the next release. They should be defined in the postsynaptic mechanisms.)

\section sect33 LEARN1SYNAPSE (Learning Synapse with a Primitive Role)
This is a simple STDP rule including a time delay for the finite transmission speed of the synapse, defined as a piecewise function.

<!--
If no spikes at t: \f$ g_{raw}(t+dt) = g_0 + (g_{raw}(t)-g_0)*\exp(-dt/\tau_{decay}) \f$
If pre or postsynaptic spike at t: \f$ g_{raw}(t+dt) = g_0 + (g_{raw}(t)-g_0)*\exp(-dt/\tau_{decay})
+A(t_{post}-t_{pre}-\tau_{decay}) \f$ 
-->
This model has 13 parameters:
- \c Erev: Reversal potential
- \c Epre: Presynaptic threshold potential
- \c tau_S: Decay time constant for S [ms]
- \c TLRN: Time scale of learning changes
- \c TCHNG: Width of learning window
- \c TDECAY: Time scale of synaptic strength decay
- \c TPUNISH10: Time window of suppression in response to 1/0
- \c TPUNISH01: Time window of suppression in response to 0/1
- \c GMAX: Maximal conductance achievable
- \c GMID: Midpoint of sigmoid g filter curve
- \c GSLOPE: Slope of sigmoid g filter curve
- \c TAUSHiFT: Shift of learning curve
- \c GSYN0: Value of syn conductance g decays to

For more details, see \cite nowotny2005self.

\section sect34 Defining a new synapse model
There is a way to define a synapse model the same way a neuronModel can be defined. This feature is still
experimental and supported only with the SpineML and development branches. 

A synapse model is a weightUpdateModel object which consists of variables, parameters, and three string objects which will be 
substituted in the code. These three strings are:

- \c simCode: Simulation code that is used when a true spike is detected (only one point after Vthresh)
- \c simCodeEvnt: Simulation code that is used for spike events (all the instances where Vm > Vthres)
- \c simLearnPost: Simulation code which is used in the learnSynapsesPost kernel/function, where postsynaptic neuron spikes before
the presynaptic neuron in the STDP window. Usually this is simply the conductance update rule defined by the other simCode elements above, with negative timing. This code is needed as simCode and simCodeEnvt are used after spanning the presynaptic spikes, where it is not possible to detect where a postsynaptic
neuron fired before the presynaptic neuron.

These codes would include update functions for adding up conductances for that neuron model and for changes in conductances for the next time step (learning). Condutance in this code should be referred to as $("G") 


\section subsect32 Connectivity types
If INDIVIDUALG is used with ALLTOALL connectivity, the conductance values are stored 
in an array called "ftype * gp<synapseName>", defined and initialised in runner.cc 
in the generated code.

If INDIVIDUALID is used with ALLTOALL connectivity, the connectivity matrix is stored 
in an array called "unsigned int * gp<synapseName>"

If connectivity is of SPARSE type, connectivity and conductance values are stored in a 
struct in order to occupy minimum memory needed. The struct Conductance contains 
3 array members and one integer member:
	1: unsigned int connN: number of connections in the population. This value is needed for allocaton of arrays. 
	2: scalar *gp: Values of conductances. 
	The indices that correspond to these values are defined in a pre-to-post basis by the following arrays:
	4: unsigned int gInd, of size connN: Indices of corresponding postsynaptic neurons concatenated for each presynaptic neuron.
	3: unsigned int *gIndInG, of size model.neuronN[model.synapseSource[synInd]]+1: This array defines from which index in the gp array the indices in gInd would correspond to the presynaptic neuron that corresponds to the index of the gIndInG array, with the number of connections being the size of gInd. More specifically, gIndIng[n+1]-gIndIng[n] would give the number of postsynaptic connections for neuron n, and the conductance values are stored in gp[gIndIng[n]] to gp[gIndIng[n]+1]. if there are no connections for a presynaptic neuron, then gp[gIndIng[n]]=gp[gIndIng[n]+1].
	
For example, consider a network of two presynaptic neurons connected to three postsynaptic neurons: 0th presynaptic neuron connected to 1st and 2nd postsynaptic neurons, the 1st presynaptic neuron connected to 0th and 2nd neurons. The struct Conductance should have these members, with indexing from 0:
	ConnN = 4
	gp= gPre0-Post1 gpre0-post2 gpre1-post0 gpre1-post2
	gInd= 1 2 0 2
	gIndIng= 0 2 4  

 See tools/gen_syns_sparse_IzhModel used in Izh_sparse project to see a working example.	

\section subsect33 Conductance definition methods
The available options work as follows:
- INDIVIDUALG: When this option is chosen in the \c
addSynapsePopulation command, GeNN reserves an array of size n_pre x
n_post float for individual conductance values for each combination of
pre and postsynaptic neuron. The actual values of the conductances are
passed at runtime from the user side code, using the \c copyGToDevice
function.

- GLOBALG: When this option is chosen, the \c addSynapsePopulation
  command must be followed within the \c modelDefinition function by a
  call to \c setSynapseG for this synapse population. This option can
  only be sensibly combined with connectivity type ALLTOALL. 

- INDIVIDUALID: When this option is chosen, GeNN expects to use the
same maximal conductance for all existing synaptic connections but
which synapses exist will be defined at runtime from the user side
code.

<br />

-----
\link sect2 Previous\endlink | \link sect3 Top\endlink | \link sect_postsyn Next\endlink

*/

//----------------------------------------------------------------------------
/*! 
\page sect_postsyn Post-synaptic integration methods

The shape of the postsynaptic current can be defined by either using a predefined method or by adding a 
new postSynModel object. 

- EXPDECAY: Exponential decay. Decay time constant and reversal potential parameters are needed for this postsynaptic mechanism.

<br />

-----
\link sect3 Previous\endlink | \link sect_postsyn  Top\endlink | \link ListOfVariables Next\endlink

*/

//----------------------------------------------------------------------------
/*! 
\page ListOfVariables Variables in GeNN

There are different types of variables that users of GeNN should be aware of and should know how to use.

\section modelVars Neuron and Synapse variables
One type of variables are those that originate from a neuron model type, weightupdate model type or post-synaptic model type. The core name of the variable is defined when the model type is introduced, i.e. with a statement such as
\code
neuronModel model;
model.varNames.push_back(String"x"));
model.varTypes.push_back(String("double"));
...
int myModel= nModels.size();
nModels.push_back(model);
\endcode
This declares that whenever the defined model type of cardinal number `myModel` is used, there will be a variable of core name `x`. The full GeNN name of this variable is obtained by directly concatenating the core name with the name of the neuron population in which the model type has been used, i.e. after a definition
\code
networkModel.addNeuronPopulation("EN", n, myModel, ...);
\endcode
there will be a variable `xEN` of type `double*` available in the global namespace of the simulation program. GeNN will pre-allocated this C array to the correct size of elements corresponding to the size of the neuron population, `n` in the example above. GeNN will also free these variables when the provided function `freeMem()` is called. Users can otherwise manipulate these variable arrays as they wish.
For convenience, GeNN provides functions `pullXXfromDevice()` and `pushXXtoDevice()` to copy the variables associated to a neuron population `XX` from the device into host memory and vice versa. E.g.
\code
pullENfromDevice();
\endcode
would copy the C array xEN from device memory into host memory (and any other variables that the neuron type of the population EN may have). 

The user can also directly use CUDA memory copy commands independent of the provided convenience function. The relevant device pointers for all variables that exist in host memory have the same name prefixed with `d_`. For example, the copy command that would be contained in `pullENfromDevice()` might look like
\code
unsigned int size;	
size = sizeof(double) * nEN;
cudaMemcpy(xEN, d_xEN, size, cudaMemcpyDeviceToHost);
\endcode
where `n` is an integer containing the population size of the EN neuron population.

The same convention as for neuron variables applies for the variables of synapse groups, both for those originating from weightupdate models and from post-synaptic models, e.g. the variables in type `NSYNAPSE` contain the variable `g` of type float. Then, after
\code
networkModel.addSynapsePopulation("ENIN", NSYNAPSE, ...);
\endcode
there will be a global variable of type `float*` with the name `gENIN` that is pre-allocated to the right size. There will also be a matching device pointer with the name `d_gENIN`. 
\note
The content of `gENIN` needs to be interpreted differently for DENSE connectivity and sparse matrix based SPARSE connectivity representations. For DENSE connectivity `gENIN` would contain "n_pre" times "n_post" elements, ordered along the pre-synaptic neuronsas the major dimension, i.e. the value of `gENIN` for the ith pre-synaptic neuron and the jth post-synaptic neuron would be `gENIN[i*n_post+j]`. The arrangement of values in the SPARSE representation is explained in section \ref subsect32

\section predefinedVars Pre-defined Variables in GeNN
In addition to the explicitly declared neuron, weightupdate and post-synaptic variables there are some variables that are built-in and modified in a predefined way by GeNN. These variables are used for intermediary calculations and communication between different parts of the generated code. They can be used in the user defined code but no other variables should be defined with these names.

- \c addtoinSyn is a local variable which is used for updating synaptic input. The way it is modified is defined by the weightupdate model,
 therefore if a user defines her own model she should update this variable to contain the input to the post-synaptic model.   
At the end of the synaptic update, its value is copied back to the d_inSyn<synapsePopulation> variables which will are used in the next step of the neuron update to provide the input to the post-snaptic neurons.

- \c Isyn is a local variable which defines the (summed) input current to a neuron. It is typically the sum of any explicit current input 
and all synaptic inputs. The way its value is calculated during the update of the postsynaptic neuron is defined by the code provided in the postSynModel. For example, the standard `expDecay` post-synaptic model defines
ps.postSyntoCurrent= String("$(inSyn)*($(E)-$(V))");
\endcode
which implements a conductance based synapse in which the post-synaptic current is given by \f$I_{\rm syn}= g*s*(V_{\rm rev}-V_{\rm post})\f$. 
\note
The `addtoinSyn` variables from all incoming synapses are automatically summed and added to the current value of `inSyn`.

The value resulting from the `postSyntoCurrent` code is assigned to `Isyn` and  can then be used in neuron simCode like so:
\code
$(V)+= (-$(V)+$(Isyn))*DT 
\endcode 

- \c sT<neuronName> is the last spike time in a neuron and is automatically generated for pre- and postsynaptic neuron groups of a synapse group i that follows a spike based learning rule (indicated by usesPostLearning[i]= TRUE for the ith synapse population).

<br />

-----
\link sect_postsyn Previous\endlink | \link ListOfVariables Top\endlink | \link  Credits Next\endlink

*/
  
//----------------------------------------------------------------------------
/*! \page Credits Credits
 
GeNN was created by Thomas Nowotny. 

Addition of Izhikevich model, debugging modes and sparse connectivity by Esin Yavuz.

Block size optimisations and delayed synapses by James Turner.

Automatic brackets and dense to sparse network conversion by Alan Diamond.

User-defined synaptic and postsynaptic methods by Alex Cope and Esin Yavuz.

<br />

-----
\link ListOfVariables Previous\endlink | \link Credits Top\endlink 

*/
//----------------------------------------------------------------------------
